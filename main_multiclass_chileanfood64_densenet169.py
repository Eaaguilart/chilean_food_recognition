import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

sns.set(style='darkgrid', palette='cubehelix')

from tensorflow.keras.models import Sequential
from keras_preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import regularizers, optimizers
from tensorflow.python.client import device_lib
from scheduler import *


root = "/home/eduardo-ucn/Documents/datasets/ChileanFood64/"

train_df = pd.read_csv(root+"trainLabels.csv",dtype=str)
test_df  = pd.read_csv(root+"testLabels.csv",dtype=str)
valid_df = pd.read_csv(root+"validLabels.csv",dtype=str)

print(train_df.head()["path"][0])

datagen = ImageDataGenerator(rescale=1./255,
                             featurewise_center=False,
                             samplewise_center=False,
                             featurewise_std_normalization=False,
                             samplewise_std_normalization=False,
                             zca_whitening=False,
                             rotation_range=20, # stratospark usa 0
                             width_shift_range=0.2,
                             height_shift_range=0.2,
                             horizontal_flip=True,
                             vertical_flip=False,
                             shear_range=0.2,
                             zoom_range=[.8, 1],
                             channel_shift_range=30,
                             fill_mode="reflect")

valid_datagen = ImageDataGenerator(rescale=1./255)

IMG_SIZE_LIST = [256, 274, 296, 342, 434]
CROP_SIZE_LIST = [224, 240, 260, 300, 380]
MODELS = ["b0","b1","b2","b3","b4"]

MODEL = "b0"
batch_size = 16
IMG_SIZE = IMG_SIZE_LIST[MODELS.index(MODEL)]
CROP_SIZE = CROP_SIZE_LIST[MODELS.index(MODEL)]

train_generator=datagen.flow_from_dataframe(dataframe=train_df,
                                            directory=root,
                                            x_col="path",
                                            y_col="label",
                                            batch_size=batch_size,
                                            seed=42,
                                            shuffle=True,
                                            class_mode="categorical",
                                            validate_filenames=False,
                                            target_size=(IMG_SIZE,IMG_SIZE))

valid_generator=valid_datagen.flow_from_dataframe(dataframe=valid_df,
                                                directory=root,
                                                x_col="path",
                                                y_col="label",
                                                batch_size=batch_size,
                                                seed=42,
                                                shuffle=False,
                                                class_mode="categorical",
                                                validate_filenames=False,
                                                target_size=(CROP_SIZE,CROP_SIZE))
test_generator=valid_datagen.flow_from_dataframe(dataframe=test_df,
                                                directory=root,
                                                x_col="path",
                                                y_col="label",
                                                batch_size=1,
                                                seed=42,
                                                shuffle=False,
                                                class_mode="categorical",
                                                validate_filenames=False,
                                                target_size= (CROP_SIZE,CROP_SIZE))    


def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]


def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)

def center_crop(x, center_crop_size, **kwargs):
    centerw, centerh = x.shape[0]//2, x.shape[1]//2
    halfw, halfh = center_crop_size[0]//2, center_crop_size[1]//2
    return x[centerw-halfw:centerw+halfw,centerh-halfh:centerh+halfh, :] 

def center_crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = center_crop(batch_x[i], (crop_length,crop_length))
        yield (batch_crops, batch_y)



train_crops = crop_generator(train_generator, crop_length=CROP_SIZE)
test_crops = crop_generator(test_generator, crop_length=CROP_SIZE)

STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size
STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size
STEP_SIZE_TEST = test_generator.n//test_generator.batch_size

print('Tamaño: {}'.format(STEP_SIZE_TRAIN))
print('Tamaño: {}'.format(STEP_SIZE_VALID))

'''
import os
from PIL import Image
folder_path = root+'train'
extensions = []
for fldr in os.listdir(folder_path):
    sub_folder_path = os.path.join(folder_path, fldr)
    for filee in os.listdir(sub_folder_path):
        file_path = os.path.join(sub_folder_path, filee)
        print('** Path: {}  **'.format(file_path), end="\r", flush=True)
        try:
            im = Image.open(file_path)
            rgb_im = im.convert('RGB')
            if filee.split('.')[1] not in extensions:
                extensions.append(filee.split('.')[1])
        except Exception as e:
            print(e)
'''

from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, DenseNet169, DenseNet121, VGG16, ResNet152, ResNet152V2, InceptionV3, ResNet50,ResNet50V2


conv_base = DenseNet169(include_top = False,
                      weights = "imagenet",
                      input_tensor = None,
                      input_shape = (CROP_SIZE, CROP_SIZE, 3),
                      classes = 64)

#conv_base = ResNet50V2(weights='imagenet', input_shape = (CROP_SIZE, CROP_SIZE, 3), classes=64, include_top=False)

def custom_scheduler(max_epochs):
    def scheduler(epoch, lr):
        if(epoch == 0):
            lr = lr
        else:
            if (epoch % 8 == 0):
                lr = (lr*0.2) #epochs comienzan en 0 , cambiar 0.2 !
        return lr
    return scheduler


DENSE_KERNEL_INITIALIZER = {
    'class_name': 'VarianceScaling',
    'config': {
        'scale': 1. / 3.,
        'mode': 'fan_out',
        'distribution': 'uniform'
    }
}



model = Sequential()
model.add(conv_base)

model.add(tf.keras.layers.GlobalAveragePooling2D(name='avg_pool'))
#model.add(tf.keras.layers.GlobalMaxPooling2D(name='max_pool'))
#model.add(Dropout(0.2, name='top_dropout'))
model.add(Dense(64,
                 activation='softmax',
                 kernel_initializer=DENSE_KERNEL_INITIALIZER,
                 name='fc_out'))

'''
learning_rate = 0.002
epochs = 10
model_name = 'EfficientNetB0'
conv_base.trainable = False

callbacks = [tf.keras.callbacks.ModelCheckpoint("chileanfood64_b0.h5", monitor="val_loss", save_best_only=True),
             tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=5)]

model.compile(loss="categorical_crossentropy",
              optimizer=optimizers.Adam(lr=learning_rate), 
              metrics=["accuracy"])

model.summary()

model.fit(train_crops, 
          steps_per_epoch = STEP_SIZE_TRAIN,
          epochs = epochs,
          validation_data = valid_generator,
          validation_steps = STEP_SIZE_VALID,
          callbacks = callbacks,
          verbose = 1)


model.load_weights("chileanfood64_b0.h5")
'''

train_generator.reset()
valid_generator.reset()
train_crops = crop_generator(train_generator, crop_length=CROP_SIZE)

#conv_base.trainable =  True
learning_rate       =  0.0002
epochs              =  40



conv_base.summary()

model.compile(loss="categorical_crossentropy",
              optimizer=optimizers.Adam(lr=learning_rate), 
              metrics=["accuracy", tf.keras.metrics.TopKCategoricalAccuracy(k=5)])


model.load_weights("chileanfood64_densenet_bs16.h5")
model.trainable=False
model.summary()
#model.evaluate(test_crops, batch_size=test_generator.batch_size, steps=STEP_SIZE_TEST)

from sklearn import metrics

def compute_statistics (y_true, y_pred, y_score):
    recall_score = round(metrics.recall_score(y_true, y_pred, average='macro'), 4)
    f1_score = round(metrics.f1_score(y_true, y_pred, average='macro'), 4)
    precision_score = round(metrics.precision_score(y_true, y_pred, average='macro'), 4)
    topk_acc = round(metrics.top_k_accuracy_score(y_true,y_score,k=5),4)
    acc = round(metrics.accuracy_score(y_true,y_pred),4)
    return recall_score, precision_score, f1_score, topk_acc, acc

y_score = model.predict_generator(test_crops, STEP_SIZE_TEST)
y_pred = np.argmax(y_score, -1)
y_true = test_generator.classes
print(np.shape(y_pred), np.shape(y_true))
print(compute_statistics (y_true, y_pred, y_score))


'''
X = next(test_crops)[0]
print(np.shape(X))
import time
init_time = time.time()
model.predict(X)
print(time.time()-init_time)
'''
'''
callbacks = [LearningRateScheduler(custom_scheduler(epochs)),
             tf.keras.callbacks.ModelCheckpoint("chileanfood64_densenet_bs16.h5", monitor="val_loss", save_best_only=True, save_weights_only=True),
             tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=8)]


model.compile(loss="categorical_crossentropy",
              optimizer=optimizers.Adam(lr=learning_rate), 
              metrics=["accuracy"])

model.fit(train_crops, 
          steps_per_epoch = STEP_SIZE_TRAIN,
          epochs=epochs,
          validation_data = valid_generator,
          validation_steps = STEP_SIZE_VALID,
          callbacks=callbacks,
          verbose=1)
'''
'''
model.load_weights("foodnonfood_maxpool.h5")

labels = (train_generator.class_indices)
labels = dict((v,k) for k,v in labels.items())


pred_indexes = list(map(lambda x: np.argmax(x), model.predict(test_generator)))
prediction_labels = list(map(lambda x: labels[x], pred_indexes))

real_labels = np.array(test_df['label'])

heat_map_data = {'y_real': real_labels, 'y_predicted': prediction_labels}

heat_map_df      = pd.DataFrame(heat_map_data, columns=['y_real','y_predicted'])
confusion_matrix = pd.crosstab(heat_map_df['y_real'], heat_map_df['y_predicted'], rownames=['Real'], colnames=['Predicted'])

plt.title('Matriz de confusión')
sns.heatmap(confusion_matrix, 
            annot=True, 
            fmt="d", 
            cmap=sns.color_palette(as_cmap=True),
            linewidths=.7, linecolor='black')
plt.show()

'''
